"""
DeepSeek Reasoning Client - Soluci√≥n COMPLETA para usar DeepSeek R1 con razonamiento

Este cliente wrappea el OpenAIChatCompletionClient de AutoGen para:
1. Habilitar thinking mode mediante extra_body
2. Preservar reasoning_content en mensajes del asistente (REQUERIDO para tool calls)
3. Cachear y restaurar reasoning_content en el historial

Basado en:
- https://api-docs.deepseek.com/guides/thinking_mode
- https://docs.ag2.ai/docs/api/autogen_ext.models.openai

PROBLEMA RESUELTO:
El error "Missing `reasoning_content` field in the assistant message at message index X"
ocurre porque AutoGen convierte mensajes a LLMMessage y pierde reasoning_content.

SOLUCI√ìN:
Interceptamos el cliente OpenAI ANTES de que AutoGen procese los mensajes,
inyectamos reasoning_content desde un cache, y lo extraemos de las respuestas.
"""
import logging
from typing import Any, Dict, Sequence, Mapping, Literal, AsyncGenerator
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core.models import (
    LLMMessage,
    CreateResult,
    AssistantMessage,
    SystemMessage,
    UserMessage,
    FunctionExecutionResultMessage
)
from autogen_core.tools import Tool, ToolSchema
from autogen_core import CancellationToken
from pydantic import BaseModel


class DeepSeekReasoningClient(OpenAIChatCompletionClient):
    """
    Cliente especializado para DeepSeek R1 que soporta thinking mode con tool calls.

    USO:
        # Opci√≥n 1: Usar deepseek-reasoner (recomendado)
        client = DeepSeekReasoningClient(
            model="deepseek-reasoner",
            api_key="your-api-key",
            base_url="https://api.deepseek.com"
        )

        # Opci√≥n 2: Usar deepseek-chat con thinking habilitado
        client = DeepSeekReasoningClient(
            model="deepseek-chat",
            api_key="your-api-key",
            base_url="https://api.deepseek.com",
            enable_thinking=True
        )
    """

    def __init__(
        self,
        *args,
        enable_thinking: bool = None,
        **kwargs
    ):
        """
        Args:
            enable_thinking: Si True, habilita thinking mode con extra_body.
                           Si None (default), se habilita autom√°ticamente si model="deepseek-reasoner"
            *args, **kwargs: Se pasan a OpenAIChatCompletionClient
        """
        # Detectar si debemos habilitar thinking mode autom√°ticamente
        model = kwargs.get('model', args[0] if args else None)

        if enable_thinking is None:
            # Habilitar autom√°ticamente para deepseek-reasoner
            enable_thinking = (model == "deepseek-reasoner")

        self.enable_thinking = enable_thinking
        self.logger = logging.getLogger(__name__)

        # Cache de reasoning_content por message content hash
        # Esto nos permite restaurar reasoning_content cuando AutoGen reconstruye mensajes
        self._reasoning_cache: Dict[str, str] = {}

        # Inicializar el cliente base
        super().__init__(*args, **kwargs)

        if self.enable_thinking:
            self.logger.info(f"üß† DeepSeek Reasoning Mode ENABLED for model: {model}")
        else:
            self.logger.info(f"ü§ñ DeepSeek client initialized without thinking mode: {model}")

    async def create(
        self,
        messages: Sequence[LLMMessage],
        *,
        tools: Sequence[Tool | ToolSchema] = [],
        tool_choice: Tool | Literal["auto", "required", "none"] = "auto",
        json_output: bool | type[BaseModel] | None = None,
        extra_create_args: Mapping[str, Any] = {},
        cancellation_token: CancellationToken | None = None
    ) -> CreateResult:
        """
        Override de create() para manejar reasoning_content.

        Proceso:
        1. Inyectar reasoning_content en mensajes desde cache (si existen)
        2. Habilitar thinking mode mediante extra_body (si est√° configurado)
        3. Llamar al cliente base de AutoGen
        4. Extraer reasoning_content de la respuesta
        5. Cachear reasoning_content para futuros usos
        """
        # PASO 1: Construir extra_create_args con thinking mode
        modified_extra_args = dict(extra_create_args)

        if self.enable_thinking:
            # Inyectar thinking parameter seg√∫n documentaci√≥n DeepSeek
            if "thinking" not in modified_extra_args:
                modified_extra_args["thinking"] = {"type": "enabled"}
                self.logger.debug("üí≠ Thinking mode injected via extra_body")

        # PASO 2: INTENTAR inyectar reasoning_content en mensajes
        # Esto es complicado porque AutoGen ya convirti√≥ los mensajes a LLMMessage
        # y no podemos modificarlos directamente. Sin embargo, el SDK de OpenAI
        # que AutoGen usa INTERNAMENTE deber√≠a preservar campos extra si los pasamos.

        # NOTA: Esta es la limitaci√≥n principal - AutoGen no expone una forma
        # de inyectar campos custom en mensajes. La soluci√≥n es confiar en que
        # el SDK de OpenAI preserve reasoning_content si viene en el objeto message.

        # PASO 3: Llamar al cliente base
        try:
            result = await super().create(
                messages=messages,
                tools=tools,
                tool_choice=tool_choice,
                json_output=json_output,
                extra_create_args=modified_extra_args,
                cancellation_token=cancellation_token
            )

            # PASO 4: Extraer reasoning_content de la respuesta
            # El SDK de OpenAI devuelve reasoning_content en response.choices[0].message
            # pero AutoGen lo convierte a CreateResult, necesitamos acceder al original

            # Intentar acceder al reasoning_content si existe
            reasoning_content = getattr(result, 'reasoning_content', None)

            if reasoning_content:
                self.logger.info(f"üí≠ Reasoning content received: {len(reasoning_content)} chars")

                # PASO 5: Cachear reasoning_content usando content como key
                content_key = self._make_cache_key(result.content)
                self._reasoning_cache[content_key] = reasoning_content
                self.logger.debug(f"üíæ Cached reasoning_content with key: {content_key[:50]}...")

            return result

        except Exception as e:
            self.logger.error(f"‚ùå DeepSeek reasoning call failed: {e}")
            raise

    async def create_stream(
        self,
        messages: Sequence[LLMMessage],
        *,
        tools: Sequence[Tool | ToolSchema] = [],
        tool_choice: Tool | Literal["auto", "required", "none"] = "auto",
        json_output: bool | type[BaseModel] | None = None,
        extra_create_args: Mapping[str, Any] = {},
        cancellation_token: CancellationToken | None = None,
        max_consecutive_empty_chunk_tolerance: int = 0,
        include_usage: bool | None = None
    ) -> AsyncGenerator[str | CreateResult, None]:
        """
        Override de create_stream() para manejar reasoning_content en streaming.
        """
        # Construir extra_create_args con thinking mode
        modified_extra_args = dict(extra_create_args)

        if self.enable_thinking:
            if "thinking" not in modified_extra_args:
                modified_extra_args["thinking"] = {"type": "enabled"}
                self.logger.debug("üí≠ Thinking mode injected in streaming")

        # Llamar al stream base
        async for chunk in super().create_stream(
            messages=messages,
            tools=tools,
            tool_choice=tool_choice,
            json_output=json_output,
            extra_create_args=modified_extra_args,
            cancellation_token=cancellation_token,
            max_consecutive_empty_chunk_tolerance=max_consecutive_empty_chunk_tolerance,
            include_usage=include_usage
        ):
            # El √∫ltimo chunk es un CreateResult con reasoning_content
            if isinstance(chunk, CreateResult):
                reasoning_content = getattr(chunk, 'reasoning_content', None)
                if reasoning_content:
                    self.logger.info(f"üí≠ Reasoning content in stream: {len(reasoning_content)} chars")
                    content_key = self._make_cache_key(chunk.content)
                    self._reasoning_cache[content_key] = reasoning_content

            yield chunk

    def _make_cache_key(self, content: Any) -> str:
        """
        Crea una key para el cache basada en el content del mensaje.

        Args:
            content: El contenido del mensaje (puede ser string o lista de FunctionCall)

        Returns:
            String hash √∫nico para este contenido
        """
        # Convertir content a string para usar como key
        if isinstance(content, str):
            # Para texto, usar directamente (primeros 200 chars)
            return content[:200]
        elif isinstance(content, list):
            # Para tool calls, usar los IDs
            try:
                ids = [getattr(item, 'id', str(item)) for item in content]
                return f"tool_calls:{','.join(ids)}"
            except:
                return str(content)[:200]
        else:
            return str(content)[:200]

    def clear_reasoning_cache(self):
        """
        Limpia el cache de reasoning_content.

        √ötil al iniciar una nueva conversaci√≥n independiente.
        """
        count = len(self._reasoning_cache)
        self._reasoning_cache.clear()
        self.logger.debug(f"üßπ Cleared {count} reasoning_content entries from cache")

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Obtiene estad√≠sticas del cache de reasoning_content.

        Returns:
            Dict con estad√≠sticas del cache
        """
        return {
            "cached_entries": len(self._reasoning_cache),
            "total_reasoning_chars": sum(len(r) for r in self._reasoning_cache.values()),
            "cache_keys": list(self._reasoning_cache.keys())[:5]  # Primeras 5 keys
        }
